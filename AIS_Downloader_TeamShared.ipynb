{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calebarr/AIS/blob/main/AIS_Downloader_TeamShared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AIS Data Downloader - Google Colab Version (Team-Ready)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the AIS downloader function\n",
        "def download_ais_data(start_date_str, end_date_str, save_folder):\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    print(f\"Files will be saved to: {save_folder}\")\n",
        "\n",
        "    # Convert string dates to datetime objects\n",
        "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
        "\n",
        "\n",
        "    # Download each file in the date range\n",
        "    for i in range((end_date - start_date).days + 1):\n",
        "        date_obj = start_date + timedelta(days=i)\n",
        "        filename = f\"AIS_{date_obj.strftime('%Y_%m_%d')}.zip\"\n",
        "        url = f\"https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2020/{filename}\"\n",
        "        file_path = os.path.join(save_folder, filename)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Downloading {filename}...\")\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"Saved: {file_path}\")\n",
        "            else:\n",
        "                print(f\"Failed: {response.status_code}\")\n",
        "        else:\n",
        "            print(f\"Already downloaded: {filename}\")\n",
        "\n",
        "# Now call the function with your chosen parameters\n",
        "download_ais_data(\n",
        "    start_date_str=\"2020-03-10\",\n",
        "    end_date_str=\"2020-04-16\",\n",
        "    save_folder=\"/content/drive/My Drive/SIADS_593/assets/raw_data\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "0yfBwBvK_XL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24493b6-2b3b-443d-c496-1dc97d311623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Files will be saved to: /content/drive/My Drive/SIADS_593/assets/raw_data\n",
            "Already downloaded: AIS_2020_03_10.zip\n",
            "Already downloaded: AIS_2020_03_11.zip\n",
            "Already downloaded: AIS_2020_03_12.zip\n",
            "Already downloaded: AIS_2020_03_13.zip\n",
            "Already downloaded: AIS_2020_03_14.zip\n",
            "Already downloaded: AIS_2020_03_15.zip\n",
            "Already downloaded: AIS_2020_03_16.zip\n",
            "Already downloaded: AIS_2020_03_17.zip\n",
            "Already downloaded: AIS_2020_03_18.zip\n",
            "Already downloaded: AIS_2020_03_19.zip\n",
            "Already downloaded: AIS_2020_03_20.zip\n",
            "Already downloaded: AIS_2020_03_21.zip\n",
            "Already downloaded: AIS_2020_03_22.zip\n",
            "Already downloaded: AIS_2020_03_23.zip\n",
            "Already downloaded: AIS_2020_03_24.zip\n",
            "Already downloaded: AIS_2020_03_25.zip\n",
            "Already downloaded: AIS_2020_03_26.zip\n",
            "Already downloaded: AIS_2020_03_27.zip\n",
            "Already downloaded: AIS_2020_03_28.zip\n",
            "Already downloaded: AIS_2020_03_29.zip\n",
            "Already downloaded: AIS_2020_03_30.zip\n",
            "Already downloaded: AIS_2020_03_31.zip\n",
            "Already downloaded: AIS_2020_04_01.zip\n",
            "Already downloaded: AIS_2020_04_02.zip\n",
            "Already downloaded: AIS_2020_04_03.zip\n",
            "Already downloaded: AIS_2020_04_04.zip\n",
            "Already downloaded: AIS_2020_04_05.zip\n",
            "Already downloaded: AIS_2020_04_06.zip\n",
            "Already downloaded: AIS_2020_04_07.zip\n",
            "Already downloaded: AIS_2020_04_08.zip\n",
            "Already downloaded: AIS_2020_04_09.zip\n",
            "Already downloaded: AIS_2020_04_10.zip\n",
            "Already downloaded: AIS_2020_04_11.zip\n",
            "Already downloaded: AIS_2020_04_12.zip\n",
            "Already downloaded: AIS_2020_04_13.zip\n",
            "Already downloaded: AIS_2020_04_14.zip\n",
            "Already downloaded: AIS_2020_04_15.zip\n",
            "Already downloaded: AIS_2020_04_16.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_zip_in_chunks(zip_path, chunksize=100_000):\n",
        "\n",
        "    all_cleaned_chunks = []\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        csv_files = [name for name in zip_ref.namelist() if name.endswith('.csv')]\n",
        "        if not csv_files:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        with zip_ref.open(csv_files[0]) as f:\n",
        "            reader = pd.read_csv(f, chunksize=chunksize)\n",
        "            for chunk in reader:\n",
        "                cleaned = extract_first_arrivals_anywhere(chunk)\n",
        "                all_cleaned_chunks.append(cleaned)\n",
        "\n",
        "    return pd.concat(all_cleaned_chunks, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "yJhRWmHVVINR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_first_arrivals_anywhere(df):\n",
        "    # Filter by relevant vessel types\n",
        "    df = df[\n",
        "        df[\"VesselType\"].isin(range(70, 90)) | df[\"VesselType\"].isin([30, 52])\n",
        "    ].copy()\n",
        "\n",
        "    # Drop rows with missing or invalid coordinates\n",
        "    df = df.dropna(subset=[\"LAT\", \"LON\"])\n",
        "    df = df[(df[\"LAT\"] != 0) & (df[\"LON\"] != 0)]\n",
        "\n",
        "    # Convert timestamps\n",
        "    df[\"BaseDateTime\"] = pd.to_datetime(df[\"BaseDateTime\"], errors='coerce')\n",
        "\n",
        "    df = df.dropna(subset=[\"BaseDateTime\"])\n",
        "\n",
        "    # Filter for ships that are of Status 1 & 5 (Anchored)\n",
        "    if \"Status\" in df.columns:\n",
        "      df = df[df[\"Status\"].isin([1, 5])]\n",
        "\n",
        "    \"\"\"\n",
        "    Working on cleaning this section\n",
        "    \"\"\"\n",
        "    # Drop all the columns that are not need\n",
        "    columns_to_drop = [\"SOG\", \"COG\", \"Heading\", \"IMO\", \"VesselName\", \"Length\", \"Width\", \"TransceiverClass\", \"Cargo\", \"CallSign\", \"Draft\"]\n",
        "    existing_cols = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "    df = df.drop(columns=existing_cols)\n",
        "\n",
        "    # End of cleaning\n",
        "\n",
        "    # Sort and get the first ping per MMSI\n",
        "    first_arrivals = (\n",
        "        df.sort_values([\"MMSI\", \"BaseDateTime\"])\n",
        "          .drop_duplicates(\"MMSI\", keep=\"first\")\n",
        "    )\n",
        "\n",
        "    # Assign port names\n",
        "    return assign_port_names(first_arrivals)\n"
      ],
      "metadata": {
        "id": "GoKkrlRTgxg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_port_names(df, buffer=1.3):\n",
        "    PORT_REGIONS = {\n",
        "        \"Los Angeles\": (33.6, 33.9, -118.5, -118.0),\n",
        "        \"Long Beach\": (33.7, 33.9, -118.25, -118.15),\n",
        "        \"Oakland\": (37.7, 37.85, -122.35, -122.2),\n",
        "        \"Seattle\": (47.5, 47.7, -122.4, -122.2),\n",
        "        \"New York\": (40.6, 40.8, -74.1, -73.9),\n",
        "        \"Norfolk\": (36.8, 37.1, -76.4, -76.2),\n",
        "        \"Savannah\": (32.0, 32.2, -81.2, -80.8),\n",
        "        \"Charleston\": (32.7, 32.9, -80.0, -79.8),\n",
        "        \"Miami\": (25.75, 25.85, -80.2, -80.0),\n",
        "        \"Port Everglades\": (26.05, 26.1, -80.15, -80.1),\n",
        "        \"Baltimore\": (39.2, 39.3, -76.6, -76.5),\n",
        "        \"Philadelphia\": (39.9, 40.0, -75.2, -75.1),\n",
        "        \"Houston\": (29.6, 29.8, -95.2, -94.8),\n",
        "        \"New Orleans\": (29.9, 30.1, -90.1, -89.9),\n",
        "        \"Jacksonville\": (30.3, 30.5, -81.7, -81.3),\n",
        "        \"San Diego\": (32.7, 32.8, -117.2, -117.1),\n",
        "        \"Boston\": (42.3, 42.4, -71.1, -70.9),\n",
        "        \"Anchorage\": (61.1, 61.3, -149.95, -149.8),\n",
        "        \"Honolulu\": (21.3, 21.4, -157.9, -157.8),\n",
        "        \"Portland\": (45.6, 45.7, -122.7, -122.6),\n",
        "        \"Puerto Rico\": (18.2, 18.3, -66.3, -66.2),\n",
        "        \"Tacoma\": (47.2, 47.4, -122.55, -122.35),\n",
        "        \"Port Arthur\": (29.85, 29.95, -93.95, -93.85),\n",
        "        \"Beaumont\": (30.0, 30.1, -94.15, -94.05),\n",
        "        \"Corpus Christi\": (27.75, 27.9, -97.45, -97.25),\n",
        "        \"Baton Rouge\": (30.4, 30.5, -91.25, -91.15),\n",
        "        \"Mobile\": (30.6, 30.7, -88.1, -88.0),\n",
        "        \"Tampa\": (27.9, 28.0, -82.5, -82.4),\n",
        "        \"San Francisco\": (37.75, 37.85, -122.45, -122.3),\n",
        "        \"Wilmington (DE)\": (39.7, 39.75, -75.55, -75.5),\n",
        "        \"Camden (NJ)\": (39.9, 39.95, -75.1, -75.05),\n",
        "        \"Providence\": (41.7, 41.8, -71.45, -71.35),\n",
        "        \"Unknown\": (None, None, None, None),  # fallback\n",
        "    }\n",
        "\n",
        "    def get_port_name(lat, lon):\n",
        "        for port, bounds in PORT_REGIONS.items():\n",
        "            if None in bounds:\n",
        "                continue  # skip 'Unknown'\n",
        "            min_lat, max_lat, min_lon, max_lon = bounds\n",
        "            if (min_lat - buffer) <= lat <= (max_lat + buffer) and \\\n",
        "               (min_lon - buffer) <= lon <= (max_lon + buffer):\n",
        "                return port\n",
        "        return \"Unknown\"\n",
        "\n",
        "    df[\"Port Name\"] = df.apply(lambda row: get_port_name(row[\"LAT\"], row[\"LON\"]), axis=1)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "Og93Hm7cBPzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_all_zips_from_folder(folder_path):\n",
        "    zip_files = sorted([\n",
        "        os.path.join(folder_path, fname)\n",
        "        for fname in os.listdir(folder_path)\n",
        "        if fname.endswith(\".zip\")\n",
        "    ])\n",
        "\n",
        "    all_dataframes = []\n",
        "    for zip_path in zip_files:\n",
        "        print(f\"Processing: {zip_path}\")\n",
        "        df = process_zip_in_chunks(zip_path)\n",
        "        all_dataframes.append(df)\n",
        "\n",
        "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "    return combined_df\n"
      ],
      "metadata": {
        "id": "sSahmkBoOGUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_dir = \"/content/drive/My Drive/SIADS_593/assets/raw_data\"\n",
        "vessel_data = concat_all_zips_from_folder(raw_data_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icpoNT6sOT-0",
        "outputId": "5c5af9db-7523-425a-90eb-106304c69a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_10.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_11.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_12.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_13.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_14.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_15.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_16.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_17.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_18.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fe2e1f857b8a>:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"Port Name\"] = df.apply(lambda row: get_port_name(row[\"LAT\"], row[\"LON\"]), axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_19.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fe2e1f857b8a>:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"Port Name\"] = df.apply(lambda row: get_port_name(row[\"LAT\"], row[\"LON\"]), axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_20.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_21.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_22.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_23.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_24.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_25.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_26.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_27.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_28.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_29.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fe2e1f857b8a>:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"Port Name\"] = df.apply(lambda row: get_port_name(row[\"LAT\"], row[\"LON\"]), axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_30.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fe2e1f857b8a>:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"Port Name\"] = df.apply(lambda row: get_port_name(row[\"LAT\"], row[\"LON\"]), axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_03_31.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_01.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_02.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_03.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_04.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_05.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_06.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_07.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ce20ab5aa661>:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[\"BaseDateTime\"] = pd.to_datetime(df[\"BaseDateTime\"], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_08.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_09.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_10.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_11.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_12.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_13.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_14.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_15.zip\n",
            "Processing: /content/drive/My Drive/SIADS_593/assets/raw_data/AIS_2020_04_16.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_save_first_arrivals(vessel_data, output_csv_path):\n",
        "    # Drop rows where 'Port Name' is 'Unknown'\n",
        "    cleaned_df = vessel_data[vessel_data['Port Name'] != 'Unknown'].copy()\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
        "\n",
        "    # Save the cleaned DataFrame to a CSV file\n",
        "    cleaned_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"Cleaned data saved to: {output_csv_path}\")\n",
        "    print(\"Shape of the cleaned data:\", cleaned_df.shape)\n",
        "\n",
        "    return cleaned_df\n"
      ],
      "metadata": {
        "id": "C2Bhh4DbOeEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_csv_path = \"/content/drive/My Drive/SIADS_593/assets/cleaned_data/first_arrivals_cleaned.csv\"\n",
        "cleaned_df = clean_and_save_first_arrivals(vessel_data, cleaned_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcXArVywPTzD",
        "outputId": "bfb4d145-4c62-46f7-a84f-2c89d2186a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to: /content/drive/My Drive/SIADS_593/assets/cleaned_data/first_arrivals_cleaned.csv\n",
            "Shape of the cleaned data: (1653365, 7)\n"
          ]
        }
      ]
    }
  ]
}